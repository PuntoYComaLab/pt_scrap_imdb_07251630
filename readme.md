# ğŸ¬ IMDb Top Movies Scraper - Arquitectura Limpia y Modular

[![Version](https://img.shields.io/badge/version-1.0.0-blue)](https://github.com/usuario/repo/releases) [![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT) [![Build](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/usuario/repo/actions)<br>**by [Javier Solis ğŸ§‘â€ğŸ’»ğŸ¤–ğŸš€ğŸ¯ğŸ”](#contact)**

Un _scraper_ robusto y escalable diseÃ±ado para extraer informaciÃ³n detallada de las pelÃ­culas mÃ¡s valoradas de IMDb. Este proyecto ha sido desarrollado siguiendo principios de **Arquitectura Limpia (Clean Architecture)**, el patrÃ³n **Page Object Model (POM)** y el **Factory Pattern**, viendo por la modularidad, reusabilidad y fÃ¡cil mantenimiento.

### ğŸ“„ Tabla de Contenidos

1.  [ğŸ“¦ Entregables](#-entregables)
2.  [ğŸ’¡ SoluciÃ³n Propuesta](#soluciÃ³n-propuesta)
    - [1ï¸âƒ£ Arquitectura y Decisiones TÃ©cnicas del Scraper Inicial](#1ï¸âƒ£-arquitectura-y-decisiones-tÃ©cnicas-del-scraper-inicial)
    - [2ï¸âƒ£ Persistencia y AnÃ¡lisis de Datos](#2ï¸âƒ£-persistencia-y-anÃ¡lisis-de-datos)
    - [3ï¸âƒ£ Proxies y Control de Red](#3ï¸âƒ£-proxies-y-control-de-red)
    - [4ï¸âƒ£ ComparaciÃ³n TÃ©cnica: Selenium o Playwright](#4ï¸âƒ£-comparaciÃ³n-tÃ©cnica-selenium-o-playwright)
3.  [ğŸš€ InstalaciÃ³n](#-instalaciÃ³n)
4.  [ğŸ¯ Uso](#-uso)
5.  [OrganizaciÃ³n del proyecto](#organizaciÃ³n-del-proyecto)
6.  [Contact](#contact)

# ğŸ“¦ Entregables

Se han preparado los siguientes entregables:

## **Repositorio en GitHub:**

- https://github.com/JavierSolis/pt_scrap_imdb_07251630

## **- Script SQL con creaciÃ³n de tablas, vistas, Ã­ndices y consultas analÃ­ticas:**

- `entregables/create.sql` (CreaciÃ³n de tablas)
- `entregables/vistas.sql`
- `entregables/indices.sql`
- `entregables/consultas_analiticas.sql`

## **Archivo CSV Generado:**

- `/data/imdb_top_movies.csv`

## **README Detallado:**

- `README.md`

## **Colaboradores de GitHub:**

- Se han invitado a los siguientes usuarios de GitHub:
  - `tc-kespejo`
  - `tc-lraigoso`
  - `sneira5`

# SoluciÃ³n propuesta

## 1ï¸âƒ£ Arquitectura y Decisiones TÃ©cnicas del Scraper Inicial

En esta secciÃ³n, se detalla la fase de concepciÃ³n y el diseÃ±o inicial del _scraper_ para la recopilaciÃ³n de pelÃ­culas top de IMDb.

- **Fase de Prueba de Concepto (POC):**

  - Se iniciÃ³ con una prueba de concepto (PoC), cuyos archivos iniciales se encuentran en la carpeta `POC/`.
  - Posteriormente, esta PoC fue refactorizada para organizar el cÃ³digo en capas claras y aplicar principios de arquitectura de software.

- **ConfiguraciÃ³n y OrquestaciÃ³n:**

  - Se implementÃ³ el archivo `config.py` para gestionar parÃ¡metros configurables del _scraper_. Inicialmente, esto incluyÃ³ la cantidad de pelÃ­culas y actores a procesar, permitiendo pruebas mÃ¡s rÃ¡pidas y mitigando el riesgo de saturar el sitio objetivo o incurrir en un bloqueo temprano.
  - El archivo `main.py` actÃºa como orquestador principal del flujo de _scraping_. Utilizando el patrÃ³n **Page Object Model (POM)**, `main` coordina las llamadas a los diferentes mÃ©todos: primero, obtiene la lista de pelÃ­culas top; luego, extrae los detalles especÃ­ficos de cada pelÃ­cula; y finalmente, persiste los datos en archivos CSV y en una base de datos SQLite.
  - Se creÃ³ un mÃ³dulo utilitario dedicado para la gestiÃ³n de logs, facilitando el seguimiento y la depuraciÃ³n del proceso.
  - Se incorporÃ³ un **`random delay`** entre las solicitudes de detalle de pelÃ­culas, para simular el comportamiento humano y reducir la probabilidad de detecciÃ³n como _bot_.

- **DiseÃ±o del Scraper (PatrÃ³n POM):**

  - Se definiÃ³ una clase abstracta `BasePage`. Esta clase encapsula la lÃ³gica comÃºn para el manejo del `BeautifulSoup` (`soup`) y la invocaciÃ³n de un `FetchFactory`, lo que permite una futura flexibilidad para cambiar el mÃ©todo de obtenciÃ³n de datos (ej. de `requests` a otra librerÃ­a).
  - BasÃ¡ndose en el patrÃ³n POM, se crearon dos clases concretas: `Home` (para la pÃ¡gina principal/listado) y `Detail` (para las pÃ¡ginas de detalle de cada pelÃ­cula), ambas heredando de `BasePage`.

- **Persistencia de Datos (PatrÃ³n Repository):**

  - Para el almacenamiento de los datos extraÃ­dos, se aplicÃ³ el patrÃ³n **Repository**.
  - Se definiÃ³ una interfaz `BaseRepository` que establece el contrato para las operaciones de guardado.
  - Actualmente, se tienen implementaciones concretas para guardar los datos en un **archivo CSV** y en una **base de datos SQLite**, demostrando la flexibilidad para persistir en mÃºltiples destinos.
  - Para facilitar las pruebas y asegurar un estado limpio en cada ejecuciÃ³n, se asumiÃ³ que los repositorios (`CSV` y `SQLite`) deben **resetearse en cada iteraciÃ³n** del _scraper_.

- **Esquema de Base de Datos:**

  - La base de datos SQLite utiliza tres tablas para almacenar la informaciÃ³n: `movies`, `movie_actor` y `actor`.
  - Esta estructura refleja una relaciÃ³n N:M (muchos a muchos) entre pelÃ­culas y actores, permitiendo que un actor participe en mÃºltiples pelÃ­culas.
  - Se decidiÃ³ comparar las entidades (ej. actores) por su nombre en lugar de la URL.

- **AnÃ¡lisis y SoluciÃ³n para la ObtenciÃ³n de Datos Completos:**

  - Al realizar pruebas para obtener 50 pelÃ­culas de la lista top, se observÃ³ que el _scraper_ solo recuperaba 25. Una investigaciÃ³n revelÃ³ que IMDb inicialmente renderiza 25 elementos en el HTML y los 225 restantes se cargan dinÃ¡micamente o estÃ¡n precargados en el DOM para una visualizaciÃ³n instantÃ¡nea mediante **GraphQL** o JSON embebido.
  - Se evaluaron las siguientes soluciones para obtener el conjunto completo de 250 pelÃ­culas:

    - **Apuntar directamente a la API GraphQL:** SerÃ­a la forma mÃ¡s limpia y eficiente de obtener datos estructurados.
    - **Utilizar el JSON embebido:** El HTML del contenedor principal de la lista de pelÃ­culas contiene un `div` con datos JSON (`<div class="__next" data-imdb-state='{"data":{"titleList"....}}'>`). Este JSON contiene los datos de las 250 pelÃ­culas.
    - **Explorar opciones de paginaciÃ³n:** Verificar si la pÃ¡gina ofrece parÃ¡metros de paginaciÃ³n en la URL o botones de "siguiente pÃ¡gina".

  - **DecisiÃ³n para la prueba:**
    - Aunque la opciÃ³n GraphQL serÃ­a la ideal por la limpieza de los datos, la prueba tÃ©cnica se enfoca en el uso de **BeautifulSoup** y **Requests** (o tÃ©cnicas de _scraping_ basadas en HTML estÃ¡tico).
    - Por lo tanto, se optarÃ¡ por **obtener los datos del JSON embebido en el HTML** (`data-imdb-state` del `div`). Esto requerirÃ¡ una refactorizaciÃ³n de la clase `Home` para extraer la informaciÃ³n de las pelÃ­culas de este JSON. Las demÃ¡s partes del _scraper_ (como la obtenciÃ³n de detalles y el guardado) no deberÃ­an verse afectadas, lo que valida la ventaja de la separaciÃ³n de responsabilidades.
  - ![json data](.readme_img/json_html.png) _(Captura de pantalla ilustrativa del JSON embebido en el HTML)_

## --

## 2ï¸âƒ£ Persistencia y AnÃ¡lisis de Datos

Esta secciÃ³n describe cÃ³mo se maneja el almacenamiento de los datos extraÃ­dos y las consultas analÃ­ticas implementadas para extraer valor de la informaciÃ³n recopilada.

- **DiseÃ±o del Modelo Entidad-RelaciÃ³n (MER):**

  - Se diseÃ±Ã³ un Modelo Entidad-RelaciÃ³n para representar la estructura de la base de datos, para tenerlo claro y realziar un mejor anÃ¡lisis.
  - ![Diagrama del MER](.readme_img/db_diagram_MER.png) _(Diagrama visual del Modelo Entidad-RelaciÃ³n de la base de datos.)_

- **ImplementaciÃ³n de la Base de Datos:**

  - Se implementÃ³ un nuevo repositorio dedicado a **MySQL**, replicando la lÃ³gica de persistencia ya existente para SQLite. Esto permite un cambio de motor de base de datos sin afectar la lÃ³gica de negocio principal del _scraper_.

- **Consultas SQL Solicitadas:**

  1.  **Obtener las 5 pelÃ­culas con mayor promedio de duraciÃ³n por dÃ©cada.**

      ```sql
      WITH RankedMovies AS (
          SELECT
              title,
              year,
              duration,
              FLOOR(year / 10) * 10 AS decada,
              ROW_NUMBER() OVER (PARTITION BY FLOOR(year / 10) * 10 ORDER BY duration DESC) AS rn
          FROM
              movies
      )
      SELECT
          title,
          year,
          duration,
          decada
      FROM
          RankedMovies
      WHERE
          rn <= 5;
      ```

  2.  **Calcular la desviaciÃ³n estÃ¡ndar de las calificaciones por aÃ±o.**

      ```sql
      SELECT
          year,
          round(STDDEV(rating),2) AS std_dev
      FROM
          movies
      GROUP BY
          year
      order by year;
      ```

  3.  **Detectar pelÃ­culas con mÃ¡s de un 20% de diferencia entre calificaciÃ³n IMDB y Metascore (normalizado).**
      _ConsideraciÃ³n:_ Se observÃ³ que el _rating_ de IMDb se presenta en una escala de 0 a 10, mientras que el Metascore va de 0 a 100. Para una comparaciÃ³n justa, el Metascore se ha normalizado dividiÃ©ndolo por 10.

      ```sql
      SELECT
        title,
        year,
        detail_url,
        rating,
        metascore,
        ROUND(ABS(rating - metascore / 10), 2) AS diferencia,
        ROUND(ABS(rating - metascore / 10) / rating * 100, 2) AS diferencia_porcentual
      FROM
        movies
      WHERE
        rating IS NOT NULL
        AND metascore IS NOT NULL
        AND rating IS NOT NULL
        AND ABS(rating - metascore / 10) / rating > 0.20
      ORDER BY diferencia_porcentual
      ```

  4.  **Crear una vista que relacione pelÃ­culas y actores, y permita filtrar por actor principal.**
      _ConsideraciÃ³n:_ Dado que no hay un campo explÃ­cito que indique al "actor principal" en la pÃ¡gina de IMDb ni en la estructura de datos obtenida, se asume para esta consulta que **el primer actor en la lista asociada a una pelÃ­cula es el actor principal.**

      ```sql
      CREATE VIEW vista_peliculas_actor_principal AS
      SELECT
      m.id AS movie_id,
      m.title,
      a.id AS actor_id,
      a.name AS actor_name
      FROM (
      SELECT
          ma.movies_id,
          ma.actors_id,
          ROW_NUMBER() OVER (PARTITION BY ma.movies_id ORDER BY ma.actors_id) AS orden
      FROM movie_actors ma
      ) sub
      JOIN movies m ON m.id = sub.movies_id
      JOIN actors a ON a.id = sub.actors_id
      WHERE sub.orden = 1;
      -- Consulta de prueba
      -- SELECT * FROM vista_peliculas_actor_principal;
      ```

  5.  **Crear un Ã­ndice o particiÃ³n si se justifica para consultas frecuentes.**
      _JustificaciÃ³n:_ Se identificÃ³ una necesidad frecuente(para mi que con esta data ahora vere que peliculas deseo ver) de consultar pelÃ­culas por su calificaciÃ³n y aÃ±o (ej., para una aplicaciÃ³n que muestre las mejores pelÃ­culas por fecha). Para optimizar el rendimiento de estas consultas, se creÃ³ un Ã­ndice.

      ```sql
      SELECT title, year, detail_url, rating FROM movies ORDER BY rating DESC, year;
      -- CreaciÃ³n del Ã­ndice
      CREATE INDEX idx_rating_year ON movies (rating DESC, year);
      -- Tiempos de ejecuciÃ³n comparativos:
      -- Antes del Ã­ndice   : 0.001749 segundos
      -- DespuÃ©s del Ã­ndice : 0.001044 segundos
      ```

## 3ï¸âƒ£ Proxies y Control de Red

Esta secciÃ³n aborda la implementaciÃ³n de estrategias robustas para la gestiÃ³n de la red y el uso de proxies, cruciales para evitar bloqueos durante el _scraping_.

- **A. Uso de proxies rotativos (mÃ­nimo 3 IPs), configurados con reintentos y _fallback_ automÃ¡tico.**
  - Se aprovechÃ³ el patrÃ³n Strategy, previamente establecido para el mecanismo de _fetch_, para implementar una estrategia de obtenciÃ³n de datos con rotaciÃ³n de proxies.
  - La clase `RotativeFetchStrategy` encapsula la lÃ³gica para gestionar un conjunto de proxies, asegurando reintentos y un _backoff_ exponencial en caso de fallos.

```python
    class RotativeFetchStrategy(FetchStrategy):
        """Estrategia con proxy rotativo con reintentos y backoff exponencial"""

        """
        218.61.37.79 443 China 660 ms
        34.41.115.197 3128 United States Council Bluffs 700 ms
        89.43.31.134 3128 Turkey 420 ms
        """

        proxies = [
            {"proxy":"http://34.41.115.197:3128","used":False},
            {"proxy":"http://89.43.31.134:3128","used":False},
            {"proxy":"http://218.61.37.79:443","used":False}
        ]

        ...
                custom_proxy = {
                    "http": proxy_url,
                    "https": proxy_url
                }
                log_info(f"Intento {i+1}/{retries} con proxy {proxy_url}")

                response = requests.get(
                    url,
                    proxies=custom_proxy,
                    headers=HEADERS,
                    timeout=SCRAPING_CONFIG['timeout'],
                    cookies=cookies
                )
```

- **DemostraciÃ³n de Funcionalidad:** Las pruebas realizadas con esta implementaciÃ³n confirman que, al detectar un error con el proxy en uso, el sistema cambia automÃ¡ticamente a otro proxy disponible en la lista, logrando obtener los datos deseados.

  - ![Registro de rotaciÃ³n de proxies](.readme_img/proxy_rotativo.png) *(Captura de pantalla del log mostrando la rotaciÃ³n y el *fallback* de proxies.)*
  - **Mejora Identificada:** Actualmente, la estrategia asigna un proxy diferente por cada solicitud individual. Una mejora futura podrÃ­a implementar un mecanismo para mantener el mismo proxy durante una sesiÃ³n.

- **B. IntegraciÃ³n con una VPN real mediante Docker, incluyendo un 'healthcheck' para validar la conexiÃ³n al paÃ­s requerido.**
  - Dada la restricciÃ³n de tiempo en esta prueba, la integraciÃ³n completa de una VPN real mediante Docker no se ha implementado en este proyecto. Sin embargo, poseo experiencia en el uso de Docker y comprendo el proceso para crear una imagen personalizada, instalar las librerÃ­as necesarias para el cliente VPN (ej., OpenVPN o WireGuard) y configurar el enrutamiento del trÃ¡fico del contenedor a travÃ©s de la VPN. Un ejemplo de implementaciÃ³n de proyectos con Docker puede verse en el repositorio: [https://github.com/JavierSolis/php-ddd-example](https://github.com/JavierSolis/php-ddd-example).
  - **Mecanismo de Healthcheck:** Para validar que la conexiÃ³n VPN se ha establecido correctamente y que la IP de salida corresponde al paÃ­s deseado, se puede utilizar un script de verificaciÃ³n. Este script podrÃ­a ejecutarse como parte del proceso de inicio del contenedor Docker o ser invocado dentro de la lÃ³gica del _fetcher_ antes de cada serie de solicitudes.
  - Adjunto una prueba conceptual del script de verificaciÃ³n de la conexiÃ³n VPN:

```python
   def check_vpn_connection(expected_country="US"):
       try:
           response = requests.get("http://ip-api.com/json/", timeout=5)
           data = response.json()
           current_ip = data.get("query")
           current_country = data.get("countryCode")
           print(f"IP actual: {current_ip}, PaÃ­s: {current_country}")
           if current_country == expected_country:
               print(f"VPN conectada correctamente al paÃ­s esperado ({expected_country}).")
               return True
           else:
               print(f"VPN NO conectada al paÃ­s esperado. PaÃ­s actual: {current_country}")
               return False
       except requests.exceptions.RequestException as e:
           print(f"No se pudo verificar la conexiÃ³n VPN: {e}")
           return False
```

- ![VerificaciÃ³n de paÃ­s de VPN](.readme_img/check_country.png) _(Captura de pantalla de la ejecuciÃ³n del script de verificaciÃ³n del paÃ­s de la VPN.)_

- **C. SimulaciÃ³n con red TOR o proxies pÃºblicos, con rotaciÃ³n controlada.**
  - La implementaciÃ³n de proxies rotativos (descrita en el punto "A") ya cubre los requisitos de simulaciÃ³n con rotaciÃ³n controlada de IPs, ya sea a travÃ©s de proxies pÃºblicos o, conceptualmente, la misma lÃ³gica aplicarÃ­a a TOR.
  - La evidencia de esta funcionalidad, incluyendo la indicaciÃ³n de la IP utilizada en cada solicitud, el registro de fallos y la demostraciÃ³n del cambio automÃ¡tico al siguiente proxy en caso de bloqueo, ha sido adjuntada previamente en la captura de pantalla del log bajo el punto "A".

---

### 4ï¸âƒ£ ComparaciÃ³n TÃ©cnica: Selenium o Playwright

La comparaciÃ³n se centrarÃ¡ principalmente en **Selenium**, con el cual tengo mayor experiencia prÃ¡ctica.

Repositorio donde lo use para QA con Docker:
https://gitlab.com/abstractapt/Store/

- **ConfiguraciÃ³n avanzada de navegador (headless, headers, evasiÃ³n de webdriver):**

  - **Modo Headless:** Selenium permite ejecutar el navegador en modo _headless_ (sin interfaz grÃ¡fica), lo que es ideal para entornos de servidor, tareas en segundo plano y despliegues en contenedores Docker, ya que no requiere un entorno visual. Esto optimiza el consumo de recursos.
  - **PersonalizaciÃ³n de Headers:** La capacidad de configurar opciones del navegador Chrome me permite personalizar cabeceras HTTP de las peticiones.
  - **EvasiÃ³n de DetecciÃ³n de WebDriver:** Es posible implementar diversas tÃ©cnicas para evitar la detecciÃ³n por parte de los sitios web. Al configurar el navegador (ej., modificando propiedades JavaScript, opciones del navegador, plugins, etx.), puedo simular un comportamiento mÃ¡s humano y sortear las defensas anti-bot que buscan comportamientos de automatizaciÃ³n.

- **Selectores dinÃ¡micos con espera explÃ­cita:**

  - A diferencia de Beautiful Soup, que solo ve el HTML inicial, Selenium interactÃºa con el DOM completo despuÃ©s de la ejecuciÃ³n de JavaScript.
  - La capacidad de **espera explÃ­cita** . Permite al _scraper_ esperar de forma inteligente segÃºn exploremos la web objetivo hasta que un elemento especÃ­fico sea visible, clickeable o estÃ© presente en el DOM. Esto resuelve problemas comunes en sitios modernos (como el observado en IMDb Top 250 donde no todos los elementos aparecen de inmediato) y evita errores por elementos no cargados.
  - [Ejemplo de espera explÃ­cita en proyecto de QA con Selenium/JS](https://gitlab.com/abstractapt/Store/-/blob/master/pages/CartPage.js?ref_type=heads#L9-20)

- **Manejo de CAPTCHA o JavaScript Rendering:**

  - **JavaScript Rendering:** Al operar sobre un navegador real, Selenium ejecuta JavaScript de forma nativa. Asi cualquier contenido generado o modificado por JS serÃ¡ visible y accesible para el _scraper_.
  - **Manejo de CAPTCHA:** Si bien Selenium no resuelve CAPTCHAs automÃ¡ticamente, permite la integraciÃ³n con servicios de terceros (ej., 2Captcha, Anti-Captcha, Kill Captcha). La estrategia consiste en detectar la presencia del CAPTCHA, obtener su identificador del formulario, enviarlo al servicio de resoluciÃ³n, esperar la respuesta, e inyectar el valor resultante en el campo correspondiente del formulario para sortear la validaciÃ³n.

- **Control de concurrencia (workers / colas):**

  - Aunque no he implementado explÃ­citamente el control de concurrencia para _scrapers_ en Python usando Selenium, sÃ­ tengo experiencia en la gestiÃ³n de concurrencia en otros lenguajes como Java y C#. La implementaciÃ³n en Python seguirÃ­a principios similares, utilizando mÃ³dulos como `threading` o `multiprocessing`, junto con estructuras de datos como `Queue` ( Colas en laravel, Servicios de amazon) y mecanismos de sincronizaciÃ³n (semÃ¡foros) para controlar el nÃºmero de navegadores concurrentes y gestionar una cola de tareas.

- **JustificaciÃ³n del uso de estas herramientas vs Scrapy:**
  - No he utilizado Scrapy directamente, pero me he documentado sobre sus capacidades (referencia: [https://docs.scrapy.org/en/latest/faq.html](https://docs.scrapy.org/en/latest/faq.html)).
  - **ElecciÃ³n de Selenium (o Playwright) por versatilidad:**
    - Aunque Scrapy es un _framework_ potente y muy eficiente para sitios web estÃ¡ticos o semi-estÃ¡ticos, mi elecciÃ³n recae en Selenium por su **versatilidad al quere ser dinÃ¡micos** o usarse con mas herramientas segÃºn he necesitado.
    - La mayorÃ­a de los sitios modernos, como IMDb, dependen de JavaScript para cargar y renderizar contenido. Scrapy, por sÃ­ solo, no ejecuta JavaScript, lo que lo limita a lo que estÃ¡ disponible en el HTML inicial.
    - Selenium, al ser un navegador real, ejecuta el JavaScript, permitiendo al _scraper_, interactuar con elementos, manejar sesiones complejas, y adaptarse a cambios de UI. Esta flexibilidad me proporciona mÃ¡s opciones para obtener los datos en un entorno web.
    - AdemÃ¡s, Selenium se integra bien con otras herramientas y configuraciones de navegador, lo que facilita la evasiÃ³n de detecciones y la simulaciÃ³n de un comportamiento mÃ¡s humano.

## ğŸš€ InstalaciÃ³n

### 1. Clona el repositorio:

```bash
git clone <repository-url>
cd PT_TechCity_Scrap
```

### 2. Instala las dependencias:

```bash
pip install -r requirements.txt
```

## ğŸ¯ Uso

### EjecuciÃ³n bÃ¡sica

```bash
python main.py
```

### Para el check del vpn

```bash
python util/checks_utils.py
```

### ConfiguraciÃ³n personalizada

Puedes modificar la configuraciÃ³n en `config.py`:

> ğŸ’¡ SegÃºn en que ambiente se despliegue convendrÃ­a mover todo a un archivo de variables de entorno

```python
# URLs
IMDB_BASE_URL = "https://www.imdb.com"
IMDB_TOP_MOVIES_URL = f"{IMDB_BASE_URL}/chart/top/"

# Headers para requests
HEADERS = {
    ...
}

# Cookies personalizadas para evitar bloqueos
CUSTOM_COOKIES = {
    ...
}

# ConfiguraciÃ³n de scraping
SCRAPING_CONFIG = {
    'max_movies': 200,  # NÃºmero mÃ¡ximo de pelÃ­culas a extraer
    'retries': 3,      # NÃºmero de reintentos para requests
    'timeout': 10,     # Timeout para requests en segundos
    'delay_min': 1,    # Delay mÃ­nimo entre requests (segundos)
    'delay_max': 3,    # Delay mÃ¡ximo entre requests (segundos)
    'max_actors': 30,   # NÃºmero mÃ¡ximo de actores a extraer por pelÃ­cula
}


# ConfiguraciÃ³n de logging
LOGGING_CONFIG = {
  ...
}
```

Para modificar el modo de hacer los request se puede agregar siguiendo patron factory en fetch_factory.py

```python
...
class FetchFactory:
    """Factory para crear estrategias de fetching"""

    @staticmethod
    def create_fetch_strategy(strategy=STANDARD_STRATEGY):
        if strategy == ROTATIVE_STRATEGY:
            return RotativeFetchStrategy()
        elif strategy == STANDARD_STRATEGY:
            return StandardFetchStrategy()
        else:
            raise ValueError(f"Estrategia desconocida: {strategy}")
...
```

## OrganizaciÃ³n del proyecto

```bash
tree -L 4
.
â”œâ”€â”€ POC
â”‚Â Â  â”œâ”€â”€ data.json
â”‚Â Â  â”œâ”€â”€ data_exporter.py
â”‚Â Â  â”œâ”€â”€ html_top.html
â”‚Â Â  â””â”€â”€ scraper_imdb.py
â”œâ”€â”€ _file
â”‚Â Â  â”œâ”€â”€ MER.mwb
â”‚Â Â  â””â”€â”€ create.sql
â”œâ”€â”€ config.py
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ backup__scrap.sql
â”‚Â Â  â”œâ”€â”€ imdb_movies.db
â”‚Â Â  â””â”€â”€ imdb_top_movies.csv
â”œâ”€â”€ entregables
â”‚Â Â  â”œâ”€â”€ create.sql
â”‚Â Â  â””â”€â”€ vistas.sql
â”œâ”€â”€ factories
â”‚Â Â  â””â”€â”€ fetch_factory.py
â”œâ”€â”€ main.py
â”œâ”€â”€ pages
â”‚Â Â  â”œâ”€â”€ base_page.py
â”‚Â Â  â”œâ”€â”€ imdb_detail_page.py
â”‚Â Â  â””â”€â”€ imdb_home_page.py
â”œâ”€â”€ readme.md
â”œâ”€â”€ repositories
â”‚Â Â  â”œâ”€â”€ base_repository.py
â”‚Â Â  â”œâ”€â”€ csv_repository.py
â”‚Â Â  â”œâ”€â”€ mysql_repository.py
â”‚Â Â  â””â”€â”€ sqlite_repository.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tasks.md
â””â”€â”€ util
    â”œâ”€â”€ checks_utils.py
    â”œâ”€â”€ converter_utils.py
    â”œâ”€â”€ logging_utils.py
    â””â”€â”€ soup_utils.py

```

---

# Contact

<div align="center">
    
   <img src=".readme_img/contact_img.png" width="90" align="center" alt="gato"/>

#### Javier Solis

ğŸ‘“ https://www.linkedin.com/in/android-developer-peru/

ğŸ’¼ https://www.behance.net/JavierJSolis

</div>
